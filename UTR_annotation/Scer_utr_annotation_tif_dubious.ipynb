{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reannotates S. cerevisiae gff using empirically discovered 3'UTRs from Pelechano et al 2013. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import gffutils\n",
    "import copy\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't think I use these at all\n",
    "# #these functions change the text in the attributes column after assigning a parent/child\n",
    "# #relationship in a gffutils database. \n",
    "# def parent_func(parent, child):\n",
    "#     #print('parent_func(%r, %r)' % (parent, child))\n",
    "#     parent.attributes['child'] = child.id\n",
    "    \n",
    "#     return parent\n",
    "    \n",
    "# def child_func(parent, child):\n",
    "#     #print('child_func(%r, %r)' % (parent, child))\n",
    "#     child.attributes['Parent'] = parent.id\n",
    "    \n",
    "#     return child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load original gff data and establish basis for filenames and directory for files.\n",
    "genome_assy_dir = os.path.normpath(\"/home/heineike/genomes/scer_20181114\")\n",
    "#os.path.normpath('C:\\\\Users\\\\BMH_work\\\\Google Drive\\\\UCSF\\\\ElSamad_Lab\\\\PKA\\\\Bioinformatics\\\\genome_assembly')\n",
    "\n",
    "sc_ref_base = 'saccharomyces_cerevisiae_R64-2-1_20150113' #'scer_ref_test'\n",
    "sc_ref_fn = genome_assy_dir + os.sep + sc_ref_base + '.gff'\n",
    "\n",
    "#utr3p_fn = genome_assy_dir + os.sep + 'Nagalakshmi_2008_3UTRs_V64.gff3'#' Nag_gff_test'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up original Scer alignment file\n",
    "\n",
    "1. By uploading via gffutils and printing again, replaces %20 with spaces, and other formatting.  Also only prints features, not sequences\n",
    "2. Change chromosome name to roman numerals which work better with the genome used by bluebee (I presume that it is from Ensembl?)\n",
    "3. Replace B\" with Bprimeprime in YNL039W\n",
    "4. For features with duplicates (which are of the following featuretypes: ['CDS', 'intron', 'noncoding_exon', 'internal_transcribed_spacer_region', 'external_transcribed_spacer_region'], replaces names with serialized version.  E.g. YLR157C-B_CDS.1 and YLR157C-B_CDS.2.   \n",
    "5. For those types of features makes ID equal to the name (after making sure Names are unique in step 4).  \n",
    "\n",
    "Reprints file as : \n",
    "\n",
    "saccharomyces_cerevisiae_R64-2-1_20150113_unique_ids.gff\n",
    "\n",
    "\n",
    "\n",
    "#Note: Originally, Two lines have the exact same region and were merged when updating the database.  Changed update to key on ID or Name and that seemed to fix it. \n",
    "\n",
    "#around this line: 14758\n",
    "#XII\tSGD\texternal_transcribed_spacer_region\t451575\t451785\t.\t-\t.\tdbxref=SGD:S000029718;Parent=ETS2-1;Name=ETS2-1_external_transcribed_spacer_region\n",
    "#XII\tSGD\texternal_transcribed_spacer_region\t451575\t451785\t.\t-\t.\tdbxref=SGD:S000006486;Parent=RDN37-1;Name=RDN37-1_external_transcribed_spacer_region\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads the original gff as a database\n",
    "\n",
    "#when troubleshooting you may need to close the database before remaking it. \n",
    "#orig_gff_db.conn.close()\n",
    "\n",
    "orig_gff_db_fn = genome_assy_dir+os.sep + sc_ref_base + '.gff'\n",
    "\n",
    "dbfn=genome_assy_dir + os.sep + sc_ref_base + '_orig.db'\n",
    "\n",
    "orig_gff_db = gffutils.create_db(orig_gff_db_fn, dbfn=dbfn, \n",
    "                                 force=True, \n",
    "                                 #keep_order=True,\n",
    "                                 #sort_attribute_values=True\n",
    "                                 merge_strategy='error')\n",
    "\n",
    "#If you don't want to recreate the database, you can load it from the file.\n",
    "#orig_gff_db = gffutils.FeatureDB(dbfn) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Chromosome name: chrXI.  New Chromosome name: XI\n",
      "Old Chromosome name: chrVII.  New Chromosome name: VII\n",
      "Old Chromosome name: chrI.  New Chromosome name: I\n",
      "Old Chromosome name: chrIII.  New Chromosome name: III\n",
      "Old Chromosome name: chrXII.  New Chromosome name: XII\n",
      "Old Chromosome name: chrV.  New Chromosome name: V\n",
      "Old Chromosome name: chrXIII.  New Chromosome name: XIII\n",
      "Old Chromosome name: chrXVI.  New Chromosome name: XVI\n",
      "Old Chromosome name: chrVI.  New Chromosome name: VI\n",
      "Old Chromosome name: chrXIV.  New Chromosome name: XIV\n",
      "Old Chromosome name: chrIX.  New Chromosome name: IX\n",
      "Old Chromosome name: chrII.  New Chromosome name: II\n",
      "Old Chromosome name: chrVIII.  New Chromosome name: VIII\n",
      "Old Chromosome name: chrIV.  New Chromosome name: IV\n",
      "Old Chromosome name: chrXV.  New Chromosome name: XV\n",
      "Old Chromosome name: chrX.  New Chromosome name: X\n",
      "Old Chromosome name: chrmt.  New Chromosome name: Mito\n"
     ]
    }
   ],
   "source": [
    "#renames all chromosomes to match the name that the SAM files from lexogen use. \n",
    "\n",
    "roman_numerals = ['I','II','III','IV','V','VI','VII','VIII','IX','X','XI','XII','XIII','XIV','XV','XVI']\n",
    "chromosome_rename_dict = {'chr' + num : num for num in roman_numerals} \n",
    "chromosome_rename_dict['chrmt']='Mito'\n",
    "\n",
    "for old_chr, new_chr in chromosome_rename_dict.items():\n",
    "    print('Old Chromosome name: ' + old_chr + '.  New Chromosome name: ' + new_chr)\n",
    "    orig_gff_db.execute(\"update features set seqid='{}' where seqid='{}'\".format(new_chr, old_chr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gffutils.create._GFFDBCreator at 0x7f53a4dcdbe0>"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removed an open quote that was contained in line 18642:\n",
    "#line = 'XIV\tSGD\tgene\t555048\t556832\t.\t+\t.\tID=YNL039W;dbxref=SGD:S000004984;Name=YNL039W;Note=Essential subunit of RNA polymerase III transcription factor (TFIIIB)%3B TFIIIB is involved in transcription of genes encoding tRNAs%2C 5S rRNA%2C U6 snRNA%2C and other small RNAs;display=Essential subunit of RNA polymerase III transcription factor (TFIIIB);Ontology_term=GO:0000126,GO:0001026,GO:0001112,GO:0001156,GO:0070896,GO:0070898;orf_classification=Verified;gene=BDP1;Alias=B\",BDP1,TFC5,TFC7,TFIIIB90,transcription factor TFIIIB subunit BDP1'\n",
    "#YNL039W, BDP1 changed B\" alias to Bprimeprime\n",
    "\n",
    "feature = orig_gff_db['YNL039W']\n",
    "att_dict = dict(feature.attributes)\n",
    "\n",
    "new_alias_list = []\n",
    "for alias in att_dict['Alias']: \n",
    "    if alias=='B\"':\n",
    "        new_alias_list.append('Bprimeprime')\n",
    "    else: \n",
    "        new_alias_list.append(alias)\n",
    "\n",
    "feature['Alias'] = new_alias_list\n",
    "\n",
    "orig_gff_db.delete(feature.id, merge_strategy='error')\n",
    "orig_gff_db.update([feature],\n",
    "                   #id_spec = ['ID','Name'],\n",
    "                   merge_strategy='error')\n",
    "\n",
    "\n",
    "#Can also do line by line in the GFF\n",
    "# with open(genome_assy_dir+os.sep + sc_ref_base + '_chr_rename.gff', 'r') as f:\n",
    "#     lines = f.readlines()\n",
    "\n",
    "# with open(genome_assy_dir+os.sep + sc_ref_base + '_chr_rename.gff', 'w') as f:\n",
    "#     for line in lines:\n",
    "#         if 'Alias=B\"' in line:\n",
    "#             line_split = line.split('Alias=B\"')\n",
    "#             line = line_split[0] + 'Alias=Bprimeprime' + line_split[1]\n",
    "#         f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print to intermediate file.  \n",
    "with open(genome_assy_dir+os.sep + sc_ref_base + '_chr_rename.gff', 'w') as outfile:\n",
    "    outfile.write('##gff-version 3\\n')\n",
    "    for feature in orig_gff_db.all_features(order_by = ('seqid','start','featuretype')):\n",
    "         print(feature,file=outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning duplicate Names\n",
      "Duplicated Names cleaned\n",
      "Assigning ID as name for the following featuretypes:\n",
      "['CDS', 'intron', 'noncoding_exon', 'internal_transcribed_spacer_region', 'external_transcribed_spacer_region']\n",
      "IDs assigned to features\n"
     ]
    }
   ],
   "source": [
    "#Identifies line items with duplicate names and replaces those with serialized names\n",
    "\n",
    "\n",
    "#Make list of all IDs.  If no ID, stores tuple with (No_ID, <feature_type>, <Name>)\n",
    "ids = []\n",
    "\n",
    "df = pd.read_table(genome_assy_dir+os.sep + sc_ref_base + '_chr_rename.gff', skiprows=1,header=None)\n",
    "\n",
    "for ind,row in df.iterrows():\n",
    "    feature_type = row[2]\n",
    "    attribs = row[8].split(';')\n",
    "    \n",
    "    att_dict = {}\n",
    "    id_found = False\n",
    "    for att in attribs: \n",
    "        att_type,att_val = att.split('=')\n",
    "        att_dict[att_type] = att_val\n",
    "        if att_type == 'ID':\n",
    "            ids.append(att_val)\n",
    "            id_found = True\n",
    "    if id_found==False:\n",
    "        if 'Name' in att_dict.keys():\n",
    "            ids.append(('No_ID',feature_type,att_dict['Name']))\n",
    "        else: \n",
    "            ids.append( 'No ID attribute. Type = ' + feature_type + '.  Other options: ' + str(att_dict))\n",
    "            raise ValueError(\"No ID or Name\")\n",
    "\n",
    "ids_counter = Counter(ids)\n",
    "\n",
    "#Makes list of tuples that represent all duplicated items in the dataset.  \n",
    "dupe_items = []\n",
    "for item in ids_counter.items():\n",
    "    if item[1]>1:\n",
    "        dupe_items.append(item)\n",
    "\n",
    "        \n",
    "\n",
    "#Cycle through duplicated items, set Names equal to new names\n",
    "print('Cleaning duplicate Names')\n",
    "features_to_update = []\n",
    "features_to_remove = []\n",
    "for ((noid, featuretype, name),NN) in dupe_items: \n",
    "    #Finds features in database that have matching Name attributes\n",
    "    cursor = orig_gff_db.execute(\"SELECT * FROM features WHERE featuretype='\" + featuretype + \"' AND attributes LIKE '%\\\"Name\\\":[\\\"\" + name + \"\\\"]%'\")\n",
    "    found_features = cursor.fetchall()\n",
    "    assert len(found_features)==NN, \"More features found than in duplicate items list\"\n",
    "    \n",
    "    #sort features with matching names by start position\n",
    "    id_start_list = []\n",
    "    for found_feature in found_features: \n",
    "        id_start_list.append((found_feature['id'], found_feature['start']))\n",
    "    id_start_list_sorted = sorted(id_start_list, key = lambda x: x[1])\n",
    "    \n",
    "    for jj, (old_id, start) in enumerate(id_start_list_sorted): \n",
    "        found_feature = orig_gff_db[old_id]\n",
    "        features_to_remove.append(orig_gff_db[old_id])\n",
    "        new_name = found_feature['Name'][0] + '.{}'.format(jj+1) \n",
    "        #found_feature['ID']=new_name  all features of selected types will have ID set to name next\n",
    "        found_feature['Name']=new_name\n",
    "        features_to_update.append(found_feature)\n",
    "    #orig_gff_db.conn.commit()\n",
    "\n",
    "orig_gff_db.delete(features_to_remove, merge_strategy='error')\n",
    "orig_gff_db.update(features_to_update, merge_strategy='error', id_spec = ['ID','Name'])\n",
    "    \n",
    "print('Duplicated Names cleaned')\n",
    "\n",
    "#cycle through all featuretypes that had duplicate Names assigned, assign the Name field as the ID\n",
    "#Could do this for all fields that don't have an ID. \n",
    "\n",
    "print('Assigning ID as name for the following featuretypes:')\n",
    "\n",
    "featuretypes_to_assign_ids = ['CDS', 'intron', 'noncoding_exon', 'internal_transcribed_spacer_region', 'external_transcribed_spacer_region']\n",
    "\n",
    "print(featuretypes_to_assign_ids)\n",
    "\n",
    "\n",
    "features_to_update = []\n",
    "features_to_remove = []\n",
    "for featuretype in featuretypes_to_assign_ids: \n",
    "    for feature in orig_gff_db.features_of_type(featuretype):\n",
    "        features_to_remove.append(orig_gff_db[feature.id])\n",
    "        feature['ID']=feature['Name'] \n",
    "        features_to_update.append(feature)\n",
    "\n",
    "orig_gff_db.delete(features_to_remove, merge_strategy='error')\n",
    "orig_gff_db.update(features_to_update, merge_strategy='error', id_spec = ['ID','Name'])\n",
    "\n",
    "print('IDs assigned to features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print to file\n",
    "with open(genome_assy_dir+os.sep + sc_ref_base + '_unique_ids.gff', 'w') as outfile:\n",
    "    outfile.write('##gff-version 3\\n')\n",
    "    for feature in orig_gff_db.all_features(order_by = ('seqid','start','featuretype')):\n",
    "         print(feature,file=outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the file looks good, close and commit\n",
    "orig_gff_db.conn.commit()\n",
    "orig_gff_db.conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Transcript annotation to SGD annotation file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For metadata at the beginning of the file: \n",
    "#Note=max extent of longest transcripts in both ypd and gal\n",
    "#Note=Difference between CDS and maximum of longest transcripts\n",
    "\n",
    "\n",
    "\n",
    "#Concatenate sgd GFF with longest transcript gff from gal and ypd\n",
    "\n",
    "#Assign transcripts as transcripts, children to the appropriate parent gene\n",
    "\n",
    "#Assign ID for long transcript as follows: \n",
    "#ID=genename_trans.long.cond.N\n",
    "#cond is gal or ypd, and number is taken from the original file eg id001\n",
    "#Note=gal count X\n",
    "\n",
    "#obtain coordinates of extent of both ypd and gal transcrips for a particular gene\n",
    "\n",
    "#make a new transcript: \n",
    "#source column: heineike_2020\n",
    "#in description column: \n",
    "#ID=genename_trans.long.max\n",
    "\n",
    "\n",
    "\n",
    "#Use that to extend the coordinates of genes\n",
    "\n",
    "\n",
    "#Subtract existing coordinates of the gene to build \n",
    "\n",
    "#source: heineike_2020\n",
    "#three_prime_utr\n",
    "#ID=genename_3pUTR.long.max or genename_5pUTR.long.max\n",
    "#child of transcript\n",
    "\n",
    "#output this file as an intermediate step to send to SGD/etc if they don't want the combined feature. \n",
    "\n",
    "#Make a transcript_region that consists of three_prime_utr and CDS in order to \n",
    "#count reads falling in the three prime region.  \n",
    "\n",
    "#source: heineike_2020\n",
    "#transcript_region\n",
    "#ID=genename_CDS.3pUTR\n",
    "#child of gene - if Parent field is filled, that will work for htseq-count\n",
    "\n",
    "#First ensure that no other features in the file consist of transcript regions.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merges two gtf files and outputs to new file\n",
    "\n",
    "merged_fn = genome_assy_dir+os.sep + sc_ref_base + '_tifs.gff'\n",
    "\n",
    "sgd_tif_max_fn = {}\n",
    "for cond in ['ypd','gal']:\n",
    "    sgd_tif_max_fn[cond] = genome_assy_dir + os.sep + 'SGD_pelechano_reanalysis' +os.sep +\"longest_full-ORF_transcripts_\" + cond + \".gff3\"\n",
    "\n",
    "with open(merged_fn, 'w') as outfile:\n",
    "    with open(genome_assy_dir+os.sep + sc_ref_base + '_unique_ids.gff') as sc_ref_file:\n",
    "        for line in sc_ref_file:\n",
    "            outfile.write(line)\n",
    "        #         #keep first line: \n",
    "#         outfile.write(sc_ref_file.readline())\n",
    "#         #drop header lines\n",
    "#         for jj in range(0,17):\n",
    "#             #print(sc_ref_file.readline())\n",
    "#             sc_ref_file.readline()\n",
    "#         for line in sc_ref_file: \n",
    "#             if line[0:3]=='chr':\n",
    "#                 outfile.write(line)\n",
    "#             else: \n",
    "#                 break\n",
    "    #add transcript data for each condition\n",
    "    for cond in ['ypd','gal']:\n",
    "        with open(sgd_tif_max_fn[cond]) as transcript_file:    \n",
    "            #drop header lines\n",
    "            for  jj in range(0,3):\n",
    "                line = transcript_file.readline()\n",
    "                #print(line)\n",
    "            for old_line in transcript_file:\n",
    "                #old_line = \"chrI\tSGD\tprimary_transcript\t143550\t147630\t.\t+\t.\tgal=1;ID=YAL002W_id001\"\n",
    "\n",
    "                linesp = old_line.split('\\t')\n",
    "\n",
    "                #rename chromosome\n",
    "                linesp[0] = chromosome_rename_dict[linesp[0]]\n",
    "\n",
    "                #reclassify source\n",
    "                linesp[1] = 'SGD_Pelechano_2013'\n",
    "                \n",
    "                #reclassify type as transcript\n",
    "                linesp[2] = 'transcript'\n",
    "                \n",
    "                \n",
    "\n",
    "                #ID= genename_trans.long.cond.sgd_id;\n",
    "                #Parent=genename;\n",
    "                #Note=cond count X\n",
    "                atts = linesp[8]\n",
    "                atts_split = atts.split(';')\n",
    "                cond, count = atts_split[0].split('=')\n",
    "                genename, sgd_id = atts_split[1].split('=')[1].split('_')\n",
    "                sgd_id = sgd_id.strip('\\n')\n",
    "                linesp[8] = 'ID={}.long.{}.{};Parent={};condition={};count={}\\n'.format(genename,cond,sgd_id,genename,cond,count)\n",
    "\n",
    "                new_line = '\\t'.join(linesp)\n",
    "\n",
    "                outfile.write(new_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads the merged.gff as a database\n",
    "\n",
    "merged_db.conn.close()    #when troubleshooting you may need to close the database before remaking it. \n",
    "\n",
    "\n",
    "merged_db = gffutils.create_db(merged_fn, dbfn=genome_assy_dir + os.sep + sc_ref_base + '_tifs.db', \n",
    "                               id_spec=('ID', 'Name'), \n",
    "                               merge_strategy='error', \n",
    "                               force=True,  #makes new database\n",
    "                               #keep_order=True, see no reason to keep this\n",
    "                               force_gff = True)\n",
    "#,\n",
    "                               #force_dialect_check = True)\n",
    "                               #sort_attribute_values=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 YDL020C\n",
      "2000 YER039C\n",
      "3000 YHR028C\n",
      "4000 YLL046C\n",
      "5000 YMR276W\n",
      "6000 YOR313C\n",
      "Updating Database with Max transcripts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gffutils.create._GFFDBCreator at 0x7f539b978748>"
      ]
     },
     "execution_count": 648,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Iterate through genes, make big transcript features\n",
    "\n",
    "#make a new transcript: \n",
    "#source column: heineike_2020\n",
    "#start and stop at the min and max of the longest transcripts for both YPD and gal.  \n",
    "#ID=genename_trans.long.max\n",
    "#remove Note from attributes\n",
    "\n",
    "max_transcripts = []\n",
    "N =1\n",
    "for feature in merged_db.features_of_type('gene'):\n",
    "    genename = feature.id\n",
    "    N = N+1\n",
    "    if np.mod(N,1000)==0:\n",
    "        print(str(N) + ' ' + genename)\n",
    "    transcript_dict = {}\n",
    "    transcripts_present=False\n",
    "    need_copy_for_max_transcript = True\n",
    "    for gene_child in merged_db.children(genename): \n",
    "        if gene_child.featuretype=='transcript':\n",
    "            transcripts_present=True\n",
    "            if need_copy_for_max_transcript:\n",
    "                max_transcript_base = gene_child\n",
    "                need_copy_for_max_transcript=False\n",
    "            assert gene_child.source == 'SGD_Pelechano_2013', 'transcript not from Pelechano 2013'\n",
    "            #dictionary is keyed on condition and has tuple representing start and end coordinates of transcript\n",
    "            genename_child, filt, cond, sgd_id = gene_child.id.split('.')\n",
    "            assert genename_child==genename, 'Child genename not equal to genename'\n",
    "            if filt=='long':\n",
    "                transcript_dict[cond] = (gene_child.start, gene_child.end)\n",
    "\n",
    "\n",
    "    if transcripts_present: \n",
    "        #Define new coords as max extend of coords in gal and YPD\n",
    "        starts = [coords[0] for coords in transcript_dict.values()]\n",
    "        ends = [coords[1] for coords in transcript_dict.values()]\n",
    "        new_coords = (min(starts),max(ends))\n",
    "        \n",
    "        #Had to use Feature class directly because I wanted to remove the Note attribute. \n",
    "        max_transcript = gffutils.Feature(seqid=max_transcript_base.seqid, \n",
    "                                    source = 'Heineike_2020',\n",
    "                                    featuretype= max_transcript_base.featuretype,\n",
    "                                    start = new_coords[0],\n",
    "                                    end = new_coords[1],\n",
    "                                    strand = max_transcript_base.strand,\n",
    "                                    frame = max_transcript_base.frame, \n",
    "                                    attributes = {'ID': [genename + '.long.max'], \n",
    "                                                  'Parent': [genename]}\n",
    "                                   )  \n",
    "        max_transcripts.append(max_transcript)\n",
    "\n",
    "print('Updating Database with Max transcripts')\n",
    "merged_db.update(max_transcripts, merge_strategy='error', id_spec='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_db.conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print to file\n",
    "with open(genome_assy_dir+os.sep + sc_ref_base + '_max_transcripts.gff', 'w') as outfile:\n",
    "    outfile.write('##gff-version 3\\n')\n",
    "    for feature in merged_db.all_features(order_by = ('seqid','start','featuretype')):\n",
    "         print(feature,file=outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 YDL019C\n",
      "2000 YER039C-A\n",
      "3000 YHR028W-A\n",
      "4000 YLL045C\n",
      "5000 YMR277W\n",
      "6000 YOR314W\n"
     ]
    }
   ],
   "source": [
    "#Cycles through all genes, makes 3'UTR and 5'UTR from max of difference between CDS\n",
    "\n",
    "#source: heineike_2020\n",
    "#CDS_3pUTR\n",
    "#ID=genename_CDS_3pUTR.long\n",
    "#child of gene - if Parent field is filled, that will work for htseq-count\n",
    "\n",
    "features_to_add = []\n",
    "N=0\n",
    "for feature in merged_db.features_of_type('gene'):\n",
    "    genename = feature.id\n",
    "    seqid = feature.seqid\n",
    "    strand = feature.strand\n",
    "    \n",
    "    N = N+1\n",
    "    if np.mod(N,1000)==0:\n",
    "        print(str(N) + ' ' + genename)\n",
    "\n",
    "    gene_data = {'CDS': []}\n",
    "    for child in merged_db.children(genename):\n",
    "        if child.featuretype=='CDS':\n",
    "            gene_data['CDS'].append(child)\n",
    "        elif child.featuretype=='transcript' and child.id.split('.')[2]=='max':\n",
    "            gene_data['max']=child\n",
    "            \n",
    "    if gene_data['CDS']==[]:\n",
    "        raise ValueError('No CDS for ' + genename)\n",
    "\n",
    "    CDS_low = min([cds.start for cds in gene_data['CDS']])\n",
    "    CDS_high = max([cds.end for cds in gene_data['CDS']])\n",
    "\n",
    "    if 'max' in gene_data.keys():\n",
    "        if strand=='-': \n",
    "            utr3p_coords = (gene_data['max'].start,CDS_low)\n",
    "            utr5p_coords = (CDS_high, gene_data['max'].end)\n",
    "        elif strand=='+':\n",
    "            utr3p_coords = (CDS_high, gene_data['max'].end)               \n",
    "            utr5p_coords = (gene_data['max'].start,CDS_low)\n",
    "\n",
    "        #Make 3'UTR features \n",
    "        utr3p = gffutils.Feature(seqid=seqid, \n",
    "                                 source = 'Heineike_2020',\n",
    "                                 featuretype = 'three_prime_utr',\n",
    "                                 start = utr3p_coords[0],\n",
    "                                 end = utr3p_coords[1],\n",
    "                                 strand = strand,\n",
    "                                 frame = '.', \n",
    "                                 attributes = {'ID': [genename + '_3pUTR.long.max'], \n",
    "                                               'Parent': [genename], \n",
    "                                               'Length': ['{}'.format(utr3p_coords[1]-utr3p_coords[0])]  \n",
    "                                              }\n",
    "                                       )\n",
    "        features_to_add.append(utr3p)\n",
    "        \n",
    "        #Make 5'UTR features \n",
    "        utr5p = gffutils.Feature(seqid=seqid, \n",
    "                                 source = 'Heineike_2020',\n",
    "                                 featuretype = 'five_prime_utr',\n",
    "                                 start = utr5p_coords[0],\n",
    "                                 end = utr5p_coords[1],\n",
    "                                 strand = strand,\n",
    "                                 frame = '.', \n",
    "                                 attributes = {'ID': [genename + '_5pUTR.long.max'], \n",
    "                                               'Parent': [genename], \n",
    "                                               'length': ['{}'.format(utr5p_coords[1]-utr5p_coords[0])]  \n",
    "                                              }\n",
    "                                       )\n",
    "\n",
    "        features_to_add.append(utr5p)\n",
    "\n",
    "\n",
    "    #Add combined CDS/3p_UTR\n",
    "    utr_flag = 'False'\n",
    "    if 'max' in gene_data.keys():\n",
    "        #If this is true then there will be a 3pUTR\n",
    "        utr_flag ='True'\n",
    "        if strand=='-':\n",
    "            coords = (utr3p_coords[0], CDS_high)\n",
    "        elif strand=='+':\n",
    "            coords = (CDS_low, utr3p_coords[1])\n",
    "    else: \n",
    "        coords = (CDS_low, CDS_high)\n",
    "\n",
    "    cds_utr3p = gffutils.Feature(seqid=seqid, \n",
    "                             source = 'Heineike_2020',\n",
    "                             featuretype = 'CDS_3pUTR',\n",
    "                             start = coords[0],\n",
    "                             end = coords[1],\n",
    "                             strand = strand,\n",
    "                             frame = '.', \n",
    "                             attributes = {'ID': [genename + '_CDS_3pUTR.long'], \n",
    "                                           'Parent': [genename], \n",
    "                                           'utr': [utr_flag]  \n",
    "                                          }\n",
    "                                   )\n",
    "    \n",
    "    features_to_add.append(cds_utr3p)  \n",
    "\n",
    "# print('Updating Database')\n",
    "# merged_db.update(max_transcripts, merge_strategy='error', id_spec='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Database with UTRs and combined CDS/UTR feature\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gffutils.create._GFFDBCreator at 0x7f539bd57240>"
      ]
     },
     "execution_count": 651,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Updating Database with UTRs and combined CDS/UTR feature')\n",
    "merged_db.update(features_to_add, merge_strategy='error', id_spec='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print to file\n",
    "with open(genome_assy_dir+os.sep + sc_ref_base + '_UTRs_pelechano_max.gff', 'w') as outfile:\n",
    "    outfile.write('##gff-version 3\\n')\n",
    "    for feature in merged_db.all_features(order_by = ('seqid','start','featuretype')):\n",
    "         print(feature,file=outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_db.conn.close()    #when troubleshooting you may need to close the database before remaking it. \n",
    "\n",
    "#Makes UTRs parents\n",
    "for utr_3p in merged_db.features_of_type('three_prime_UTR'):\n",
    "    gene_id = utr_3p.id.split('_')[0]\n",
    "    #print(gene_id)\n",
    "    #print(utr_3p.id)\n",
    " \n",
    "    try:\n",
    "        merged_db.add_relation(gene_id,utr_3p, 1, child_func = child_func, parent_func=parent_func)\n",
    "#        print(utr_3p.attributes)\n",
    "    except gffutils.FeatureNotFoundError:\n",
    "        print('There is no matching orf for the 3prime UTR ' + gene_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(genome_assy_dir+os.sep + sc_ref_base + '_tiftest.gff', 'w') as outfile:\n",
    "    outfile.write('##gff-version 3\\n')\n",
    "    for feature in merged_db.all_features(order_by = ('seqid','start','featuretype')):\n",
    "         print(feature,file=outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Uses merge_Nag_scer64.sh to \n",
    "#sort combined UTR and annotation file and then merges the coordinates of the UTR and previous gene to get new coordinates for gene. \n",
    "#with bedtools\n",
    "merge_cmd = ['/home/heineike/github/UTR_annotation/UTR_annotation/merge_Nag_scerR64.sh',\n",
    "             '/home/heineike/genomes/scer_20181114/saccharomyces_cerevisiae_R64-2-1_20150113']\n",
    "\n",
    "os.system(' '.join(merge_cmd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build dict of coordinates that need to be changed\n",
    "merge_table = pd.read_table(genome_assy_dir+os.sep + sc_ref_base + '_nagdata_UTRchildren_merged', header = None)\n",
    "\n",
    "coord_change_dict = {}\n",
    "\n",
    "for row in merge_table.iterrows():\n",
    "    annotation = row[1][5]\n",
    "    annotation_ids = [item.split(\"=\")[1] for item in annotation.split(\";\") if item.split(\"=\")[0]==\"ID\"]\n",
    "    for ann_id in annotation_ids: \n",
    "        if '_' in ann_id:\n",
    "            if '3UTR' == ann_id.split('_')[1]:\n",
    "                gene_id = ann_id.split('_')[0]\n",
    "                coord_change = {}\n",
    "    \n",
    "                #for some reason the start coordinate for merged items on the + strand\n",
    "                #had one number subtracted in bedtools coord_change['start'] = row[1][1]+1\n",
    "                if row[1][3]=='+':\n",
    "                    coord_change['start'] = row[1][1]+1\n",
    "                elif row[1][3]=='-':\n",
    "                    coord_change['start'] = row[1][1]\n",
    "                coord_change['end'] = row[1][2]\n",
    "                coord_change['UTR_id'] = ann_id\n",
    "\n",
    "                coord_change_dict[gene_id] = coord_change\n",
    "\n",
    "\n",
    "#coord_change_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load new database that is sorted from bedtools\n",
    "#merged_sorted_db.conn.close()   #when troubleshooting may need to close database before reloading\n",
    "\n",
    "merged_sorted_fn = genome_assy_dir+os.sep + sc_ref_base + '_nagdata_UTRchildren_sorted.gff'\n",
    "\n",
    "\n",
    "merged_sorted_db = gffutils.create_db(merged_sorted_fn, dbfn=genome_assy_dir + os.sep + sc_ref_base + '_nagdata_UTRchildren_sorted.db', force=True, keep_order=True, \n",
    "                        merge_strategy='merge', sort_attribute_values=True)\n",
    "\n",
    "# merged_sorted_db.schema()\n",
    "# cursor = merged_sorted_db.execute(\"select id from features where seqid = 'I'\")\n",
    "# row = cursor.fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Chromosome name: chrmt.  New Chromosome name: Mito\n",
      "Old Chromosome name: chrIV.  New Chromosome name: IV\n",
      "Old Chromosome name: chrXIV.  New Chromosome name: XIV\n",
      "Old Chromosome name: chrXIII.  New Chromosome name: XIII\n",
      "Old Chromosome name: chrVIII.  New Chromosome name: VIII\n",
      "Old Chromosome name: chrXI.  New Chromosome name: XI\n",
      "Old Chromosome name: chrXV.  New Chromosome name: XV\n",
      "Old Chromosome name: chrX.  New Chromosome name: X\n",
      "Old Chromosome name: chrI.  New Chromosome name: I\n",
      "Old Chromosome name: chrIII.  New Chromosome name: III\n",
      "Old Chromosome name: chrXVI.  New Chromosome name: XVI\n",
      "Old Chromosome name: chrVII.  New Chromosome name: VII\n",
      "Old Chromosome name: chrXII.  New Chromosome name: XII\n",
      "Old Chromosome name: chrV.  New Chromosome name: V\n",
      "Old Chromosome name: chrVI.  New Chromosome name: VI\n",
      "Old Chromosome name: chrII.  New Chromosome name: II\n",
      "Old Chromosome name: chrIX.  New Chromosome name: IX\n"
     ]
    }
   ],
   "source": [
    "#renames all chromosomes to match the name that the SAM files from lexogen use. \n",
    "roman_numerals = ['I','II','III','IV','V','VI','VII','VIII','IX','X','XI','XII','XIII','XIV','XV','XVI']\n",
    "chromosome_rename_dict = {'chr' + num : num for num in roman_numerals} \n",
    "chromosome_rename_dict['chrmt']='Mito'\n",
    "\n",
    "for old_chr, new_chr in chromosome_rename_dict.items():\n",
    "    print('Old Chromosome name: ' + old_chr + '.  New Chromosome name: ' + new_chr)\n",
    "    merged_sorted_db.execute(\"update features set seqid='{}' where seqid='{}'\".format(new_chr, old_chr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update features set end=68527 where id = 'YNL299W'\n",
      "update features set end=509280 where id = 'YMR120C'\n",
      "update features set end=818438 where id = 'YOR262W'\n",
      "update features set end=744305 where id = 'YOR212W'\n",
      "update features set end=51002 where id = 'YBL089W'\n"
     ]
    }
   ],
   "source": [
    "#Moves start and end locations for each gene per new file\n",
    "jj = 0\n",
    "for gene_id, coord_change in coord_change_dict.items():\n",
    "    new_start = coord_change['start']\n",
    "    new_end = coord_change['end']\n",
    "    #prints out update statement every 1000 iterations. \n",
    "    jj = jj + 1\n",
    "    if jj==1000:\n",
    "        print(\"update features set end={} where id = '{}'\".format(new_end,gene_id))\n",
    "        jj = jj-1000\n",
    "    merged_sorted_db.execute(\"update features set start={} where id = '{}'\".format(new_start,gene_id))\n",
    "    merged_sorted_db.execute(\"update features set end={} where id = '{}'\".format(new_end,gene_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print to file\n",
    "with open(genome_assy_dir+os.sep + sc_ref_base + '_UTRs.gff', 'w') as outfile:\n",
    "    outfile.write('##gff-version 3\\n')\n",
    "    for feature in merged_sorted_db.all_features():\n",
    "         print(feature,file=outfile)\n",
    "\n",
    "#In the backup file the child tag was at the end of many of the lines.  This most recent time I ran it, \n",
    "#the child tag was in the middle. Also the size was slightly different because the backup had windows CR LF instead of unix LF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after the file was printed manually removed an open quote that was contained in line 22795:\n",
    "#line = 'XIV\tSGD\tgene\t555048\t556886\t.\t+\t.\tID=YNL039W;dbxref=SGD:S000004984;Name=YNL039W;Note=Essential subunit of RNA polymerase III transcription factor (TFIIIB)%3B TFIIIB is involved in transcription of genes encoding tRNAs%2C 5S rRNA%2C U6 snRNA%2C and other small RNAs;display=Essential subunit of RNA polymerase III transcription factor (TFIIIB);Ontology_term=GO:0000126,GO:0001026,GO:0001112,GO:0001156,GO:0070896,GO:0070898;orf_classification=Verified;child=YNL039W_3UTR;gene=BDP1;Alias=B\",BDP1,TFC5,TFC7,TFIIIB90,transcription factor TFIIIB subunit BDP1'\n",
    "#YNL039W, BDP1 changed B\" alias to Bprimeprime\n",
    "\n",
    "with open(genome_assy_dir+os.sep + sc_ref_base + '_UTRs.gff', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "with open(genome_assy_dir+os.sep + sc_ref_base + '_UTRs.gff', 'w') as f:\n",
    "    for line in lines:\n",
    "        if 'Alias=B\"' in line:\n",
    "            line_split = line.split('Alias=B\"')\n",
    "            line = line_split[0] + 'Alias=Bprimeprime' + line_split[1]\n",
    "        f.write(line)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the output looked good, commit and print to file\n",
    "merged_sorted_db.conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some useful commands to access / change data in gffutils. \n",
    "\n",
    "#db_id = 'YEL058W'\n",
    "#cursor = merged_db.execute('select * from features where id =\"%s\"' % db_id)\n",
    "#row = cursor.fetchone()\n",
    "#row['end']\n",
    "\n",
    "#feat = list(merged_db.features_of_type('three_prime_UTR'))[0]\n",
    "\n",
    "\n",
    "#pd.read_sql('select * from features;', merged_db.conn)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
